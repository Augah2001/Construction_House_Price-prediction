{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"id":"PtfuYoJnm5U3"},"outputs":[{"data":{"text/plain":["array([[-1.34258533e+00, -2.54148127e-01, -2.36183364e-01,\n","        -1.99336874e-01],\n","       [-1.94073581e-02,  4.57935878e-02, -1.04808843e-01,\n","        -4.41592873e-01],\n","       [-7.26149301e-01, -2.33159541e-01, -5.86607909e-01,\n","        -4.33925667e-01],\n","       [ 1.43333997e+00, -1.35154104e-01,  1.13100291e-01,\n","        -5.86168025e-01],\n","       [-2.54988006e-01, -1.47859833e-01, -2.84452908e-01,\n","        -5.10010739e-01],\n","       [ 7.26598026e-01, -4.86227236e-02, -8.47086749e-02,\n","        -4.24272925e-01],\n","       [-3.33514888e-01, -1.80980112e-01, -1.69287750e-01,\n","        -3.96840455e-01],\n","       [ 1.11923244e+00, -2.19530894e-01, -9.99785946e-02,\n","        -5.32664539e-01],\n","       [ 9.85736738e-01, -1.70343145e-01, -1.32450533e-01,\n","         2.39636893e+00],\n","       [-3.31753642e+00,  6.12868478e+00,  6.04377431e+00,\n","        -1.12446971e+00],\n","       [-1.27976382e+00, -2.57543307e-01, -4.06964161e-01,\n","         3.45850922e-01],\n","       [ 5.69544261e-01, -2.19303508e-01, -2.01322129e-02,\n","        -8.21645904e-01],\n","       [ 8.44388350e-01, -8.48227395e-02, -5.47136241e-02,\n","        -5.02251445e-01],\n","       [ 8.83651791e-01, -2.91657840e-01, -5.11129339e-01,\n","        -1.04035504e+00],\n","       [ 4.35911765e-17,  1.26719381e-16, -1.80044481e-16,\n","         0.00000000e+00],\n","       [-8.43939625e-01, -2.33537403e-01, -3.43227687e-01,\n","        -4.02981242e-01],\n","       [ 1.47260341e+00, -2.55637280e-01, -4.21217036e-01,\n","        -1.11057692e+00],\n","       [-2.90325103e-01, -2.75715209e-01, -2.40555294e-01,\n","         1.03359057e+00],\n","       [ 4.35911765e-17,  1.26719381e-16, -1.80044481e-16,\n","         0.00000000e+00],\n","       [ 1.11923244e+00, -1.99485289e-01,  3.01150400e-02,\n","        -8.98908253e-01],\n","       [ 7.69787811e-01, -1.05359898e-01,  1.36269647e-01,\n","         3.03930647e+00],\n","       [-1.59387135e+00, -1.39827770e-01, -2.26038893e-01,\n","        -3.27454002e-01],\n","       [-6.65234876e-02, -2.99970791e-01, -5.32471290e-02,\n","         2.05585172e+00],\n","       [ 1.11923244e+00, -2.32710343e-01, -3.44921283e-01,\n","        -5.15267741e-01],\n","       [-1.47215468e+00, -1.97192484e-01, -3.73580335e-01,\n","        -3.62828618e-01],\n","       [-4.12041771e-01, -2.35612854e-01, -1.12441586e-01,\n","        -3.75522272e-01],\n","       [-6.86885860e-01, -2.35739923e-01, -1.32119859e-01,\n","        -3.78299505e-01],\n","       [ 1.19775932e+00,  1.18170212e-01, -3.63114691e-01,\n","        -6.05121055e-01],\n","       [ 3.73227054e-01,  5.56527932e-03, -3.06402824e-01,\n","        -4.90650930e-01],\n","       [-7.37928333e-01, -2.45394895e-01, -5.74282373e-03,\n","         9.10240188e-01],\n","       [ 4.35911765e-17,  1.26719381e-16, -1.80044481e-16,\n","         0.00000000e+00]])"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.regularizers import l2\n","\n","# Load the dataset\n","data = pd.read_csv('bmiprediction.csv')\n","\n","# Handling missing values: Fill missing values with the mean of the column\n","data.fillna(data.mean(), inplace=True)\n","\n","# Separate features and target variable\n","X = data.drop(columns=['BMI'])\n","y = data['BMI']\n","\n","# Log transform the target variable to reduce skewness\n","y_log = np.log1p(y)\n","\n","# Identify numerical columns (all columns in this case)\n","numerical_features = X.columns.tolist()\n","\n","# Preprocess the data: StandardScaling for numerical features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Ensure the preprocessed data is dense\n","X_scaled_dense = X_scaled if isinstance(X_scaled, np.ndarray) else X_scaled.toarray()\n","\n","# Split the data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled_dense, y_log, test_size=0.2, random_state=42)\n","\n","# Define KFold cross-validation\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# Convert y_train to a numpy array\n","y_train = y_train.to_numpy()\n","\n","# Create a function to build the model\n","# def build_model():\n","#     model = Sequential([\n","#         Dense(256, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(0.01)),\n","#         Dropout(0.3),\n","#         BatchNormalization(),\n","#         Dense(128, activation='tanh', kernel_regularizer=l2(0.01)),\n","#         Dropout(0.3),\n","#         BatchNormalization(),\n","#         Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n","#         Dropout(0.3),\n","#         Dense(1)\n","#     ])\n","#     optimizer = Adam(learning_rate=0.001)\n","#     model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_absolute_error'])\n","#     return model\n","\n","# # Train and evaluate the model using cross-validation\n","# mse_scores = []\n","# mae_scores = []\n","\n","# for train_index, val_index in kf.split(X_train):\n","#     X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n","#     y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n","    \n","#     model = build_model()\n","    \n","#     early_stopping = EarlyStopping(patience=20, restore_best_weights=True)\n","#     reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.00001)\n","    \n","#     history = model.fit(X_train_fold, y_train_fold, epochs=200, validation_data=(X_val_fold, y_val_fold), callbacks=[early_stopping, reduce_lr], verbose=0)\n","    \n","#     val_mse, val_mae = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n","#     mse_scores.append(val_mse)\n","#     mae_scores.append(val_mae)\n","\n","# # Average scores\n","# avg_mse = np.mean(mse_scores)\n","# avg_mae = np.mean(mae_scores)\n","\n","# print(f\"Average Mean Squared Error: {avg_mse}\")\n","# print(f\"Average Mean Absolute Error: {avg_mae}\")\n","\n","# # Train the final model on the entire training set\n","# model = build_model()\n","# history = model.fit(X_train, y_train, epochs=1000, validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr], verbose=0)\n","\n","# # Evaluate the model on the test data\n","# test_mse, test_mae = model.evaluate(X_test, y_test, verbose=0)\n","\n","# print(\"Test Mean Squared Error:\", test_mse)\n","# print(\"Test Mean Absolute Error:\", test_mae)\n","\n","# # Predict individually for each row in the dataset\n","# predictions_exp = []\n","\n","# for i in range(X.shape[0]):\n","#     sample_data = X.iloc[i:i+1, :]  # Take one row at a time\n","#     sample_data_scaled = scaler.transform(sample_data)\n","#     sample_prediction = model.predict(sample_data_scaled)\n","#     sample_prediction_exp = np.expm1(sample_prediction)\n","#     predictions_exp.append(sample_prediction_exp[0][0])\n","\n","# # Inverse log transform the actual values\n","# y_exp = np.expm1(y_log)\n","\n","# # Create a DataFrame for actual vs predicted values\n","# comparison_df = pd.DataFrame({'Actual': y_exp, 'Predicted': predictions_exp})\n","\n","# # Define the function to plot and generate table\n","# def plot_and_table():\n","#     plt.figure(figsize=(10, 6))\n","#     plt.plot(comparison_df['Actual'], label='Actual BMPI')\n","#     plt.plot(comparison_df['Predicted'], label='Predicted BMPI')\n","#     plt.xlabel(\" Dataset Index\")\n","#     plt.ylabel(\"BMPI\")\n","#     plt.title(\"Actual vs. Predicted BMPI\")\n","#     plt.legend()\n","#     plt.show()\n","    \n","#     print(comparison_df)\n","\n","# # Call the function to plot and print the table\n","# plot_and_table()"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n"]},{"data":{"text/plain":["array([[-0.22118059],\n","       [ 0.00630469],\n","       [-0.10254487],\n","       [-0.05461688],\n","       [-0.22526953],\n","       [-0.11517841],\n","       [-0.05912982],\n","       [-0.09314869]], dtype=float32)"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["model.predict(X_test)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["0     4.304065\n","1     4.369448\n","2     4.398146\n","3     4.421247\n","4     4.409155\n","5     4.460144\n","6     4.513055\n","7     4.535820\n","8     4.535820\n","9     4.623010\n","10    4.614130\n","11    4.637637\n","12    4.623992\n","13    4.627910\n","14    4.641502\n","15    4.638605\n","16    4.638605\n","17    4.662495\n","18    4.641502\n","19    4.696837\n","20    4.694096\n","21    4.653008\n","22    4.643429\n","23    4.624973\n","24    4.615121\n","25    4.497585\n","26    4.422449\n","27    4.675442\n","28    4.556610\n","29    4.542976\n","30    4.585885\n","31    4.628789\n","32    4.644295\n","33    4.565597\n","34    4.354013\n","35    4.568207\n","36    4.568207\n","37    4.568207\n","38    4.568207\n","Name: BMI, dtype: float64"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.regularizers import l2\n","\n","# Load the dataset\n","file_path = \"bmiprediction.csv\"\n","data = pd.read_csv(file_path)\n","\n","# Handling missing values: Fill missing values with the mean of the column\n","data.fillna(data.mean(), inplace=True)\n","\n","# Separate features and target variable\n","X = data.drop(columns=['BMI'])\n","y = data['BMI']\n","\n","# Log transform the target variable to reduce skewness\n","y = np.log1p(y)\n","\n","# Identify numerical columns (all columns in this case)\n","numerical_features = X.columns.tolist()\n","\n","# Preprocess the data: StandardScaling for numerical features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Ensure the preprocessed data is dense\n","X_scaled_dense = X_scaled if isinstance(X_scaled, np.ndarray) else X_scaled.toarray()\n","\n","# Split the data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled_dense, y, test_size=0.2, random_state=42)\n","\n","# Define KFold cross-validation\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# Convert y_train to a numpy array\n","# y_train = y_train.to_numpy()\n","\n","# Create a function to build the model\n","# def build_model():\n","#     model = Sequential([\n","#     Dense(256, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(0.01)),\n","#      Dropout(0.3),\n","#      BatchNormalization(),\n","#      Dense(128, activation='tanh', kernel_regularizer=l2(0.01)),\n","#      Dropout(0.3),\n","#      BatchNormalization(),\n","#      Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n","#      Dropout(0.3),\n","#       Dense(1)\n","#   ])\n","#     optimizer = Adam(learning_rate=0.001)\n","#     model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_absolute_error'])\n","#     return model\n","\n","# # Train and evaluate the model using cross-validation\n","# mse_scores = []\n","# mae_scores = []\n","\n","# for train_index, val_index in kf.split(X_train):\n","#     X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n","#     y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n","\n","#     model = build_model()\n","\n","#     early_stopping = EarlyStopping(patience=20, restore_best_weights=True)\n","#     reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.00001)\n","\n","#     history = model.fit(X_train_fold, y_train_fold, epochs=1000, validation_data=(X_val_fold, y_val_fold), callbacks=[early_stopping, reduce_lr], verbose=0)\n","\n","#     val_mse, val_mae = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n","#     mse_scores.append(val_mse)\n","#     mae_scores.append(val_mae)\n","\n","# # Average scores\n","# avg_mse = np.mean(mse_scores)\n","# avg_mae = np.mean(mae_scores)\n","\n","# print(f\"Average Mean Squared Error: {avg_mse}\")\n","# print(f\"Average Mean Absolute Error: {avg_mae}\")\n","\n","# Evaluate the model on the test data\n","# model = build_model()\n","# history = model.fit(X_train, y_train, epochs=1000, validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr], verbose=0)\n","# test_mse, test_mae = model.evaluate(X_test, y_test, verbose=0)\n","\n","# print(\"Test Mean Squared Error:\", test_mse)\n","# print(\"Test Mean Absolute Error:\", test_mae)\n","\n","# # Predict on train and test data\n","# y_train_pred = model.predict(X_train).flatten()\n","# y_test_pred = model.predict(X_test).flatten()\n","\n","# Inverse log transform the predictions and the actual values\n","# y_train_exp = np.expm1(y_train)\n","# y_train_pred_exp = np.expm1(y_train_pred)\n","# y_test_exp = np.expm1(y_test)\n","# y_test_pred_exp = np.expm1(y_test_pred)\n","\n","# # Create a DataFrame for actual vs predicted values for training set\n","# train_comparison_df = pd.DataFrame({'Actual': y_train_exp, 'Predicted': y_train_pred_exp})\n","\n","# # Create a DataFrame for actual vs predicted values for test set\n","# test_comparison_df = pd.DataFrame({'Actual': y_test_exp, 'Predicted': y_test_pred_exp})\n","\n","# # Concatenate train and test comparison DataFrames\n","# comparison_df = pd.concat([train_comparison_df, test_comparison_df], keys=['Train', 'Test'])\n","\n","# # Calculate accuracy for each prediction (1 - absolute percentage error)\n","# accuracy_train = 1 - np.abs((y_train_exp - y_train_pred_exp) / y_train_exp)\n","# accuracy_test = 1 - np.abs((y_test_exp - y_test_pred_exp) / y_test_exp)\n","\n","# # Average accuracy\n","# average_accuracy_train = np.mean(accuracy_train)\n","# average_accuracy_test = np.mean(accuracy_test)\n","# average_accuracy = np.mean([average_accuracy_train, average_accuracy_test])\n","\n","# print(\"Average Accuracy (Training):\", average_accuracy_train)\n","# print(\"Average Accuracy (Testing):\", average_accuracy_test)\n","# print(\"Overall Average Accuracy:\", average_accuracy)\n","\n","# # Visualize actual vs. predicted values\n","# plt.figure(figsize=(10, 6))\n","# plt.scatter(y_test_exp, y_test_pred_exp, alpha=0.5)\n","# plt.xlabel(\"Actual BMI\")\n","# plt.ylabel(\"Predicted BMI\")\n","# plt.title(\"Actual vs. Predicted BMI\")\n","# plt.plot([y_test_exp.min(), y_test_exp.max()], [y_test_exp.min(), y_test_exp.max()], 'k--', lw=2)\n","# plt.show()\n","\n","# # Plot training history\n","# history_dict = history.history\n","\n","# # Check if 'loss' and 'val_loss' are in the history dictionary\n","# if 'loss' in history_dict and 'val_loss' in history_dict:\n","#     loss_values = history_dict['loss']\n","#     val_loss_values = history_dict['val_loss']\n","#     epochs = range(1, len(loss_values) + 1)\n","\n","#     plt.figure(figsize=(10, 6))\n","#     plt.plot(epochs, loss_values, 'bo', label='Training loss')\n","#     plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n","#     plt.title('Training and validation loss')\n","#     plt.xlabel('Epochs')\n","#     plt.ylabel('Loss')\n","#     plt.legend()\n","#     plt.show()\n","# else:\n","#     print(\"No loss data found in history.\")\n","\n","# # Print comparison table\n","# print(comparison_df)\n","\n","# # Save the comparison table to a CSV file\n","# comparison_df.to_csv(\"actual_vs_predicted_bmi.csv\")\n","\n","# # Test the model with a sample data\n","# sample_data = np.array([[2.5, 500, 300, 1200000]])\n","# sample_data_scaled = scaler.transform(sample_data)\n","# sample_prediction = model.predict(sample_data_scaled)\n","# sample_prediction_exp = np.expm1(sample_prediction)\n","# print(\"Sample Data Prediction (after inverse transform):\", sample_prediction_exp)\n","y\n","\n"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.55918546649421\n"]}],"source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.linear_model import LassoCV\n","from sklearn.model_selection import cross_val_score\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.svm import SVR\n","import xgboost as xgb\n","\n","\n","kNeighbors = KNeighborsRegressor()\n","svm = SVR()\n","LinearRegression = LassoCV()\n","randomForest = RandomForestRegressor()\n","xgboost = xgb.XGBRegressor(learning_rate=0.199999999999, n_estimators=184)\n","\n","models = [{'name': \"kNeighbors\", \"model\": kNeighbors},\n","          {\"name\": \"support vector machine\", \"model\": svm},\n","\n","          {\"name\": \"LassoCV regression\", \"model\": LinearRegression},\n","          {\"name\": \"random forest\", \"model\": randomForest\n","           }, {\"name\": \"xgboost\", \"model\": xgboost}]\n","\n","\n","\n","\n","randomForest.fit(X_train,y_train)\n","y_preds = randomForest.predict(X_test)\n","\n","# pd.DataFrame({'y_pred': np.expm1(y_preds), 'y_test': np.expm1(y_test)})\n","\n","result_dict = []\n","sum = 0\n","avg = 0\n","for i in range(0,5):\n","    \n","    score = randomForest.score(X_test, y_test)\n","    sum+= score\n","    result_dict.append(score)\n","    avg = sum/ len(result_dict)\n","\n","print(avg)    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":120605,"status":"ok","timestamp":1717964630908,"user":{"displayName":"Robert tafara","userId":"00919811940274385767"},"user_tz":-120},"id":"NUJQ5OmIpCm9","outputId":"6cc10325-fecf-400c-fb77-5f941b37dd5a"},"outputs":[],"source":["import pickle\n","\n","with open('model1.pkl', 'wb') as f:\n","    pickle.dump(randomForest, f)\n","with open('scaler.pkl', 'wb') as f:\n","    pickle.dump(scaler, f)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["array([[ 6.79481896e-01, -1.14901177e-01, -1.68083513e-01,\n","         2.24937496e+00],\n","       [ 4.35911765e-17,  1.26719381e-16, -1.80044481e-16,\n","         0.00000000e+00],\n","       [ 7.65861467e-01, -1.55003532e-01,  4.14415938e-01,\n","        -8.99529023e-01],\n","       [ 7.26598026e-01, -1.96522588e-01, -2.77747573e-01,\n","        -5.06669711e-01],\n","       [-2.51061662e-01, -2.38503104e-01, -2.44269992e-01,\n","         1.42925560e+00],\n","       [-9.10687475e-01, -3.11580834e-01, -4.16347196e-01,\n","         4.86981033e-01],\n","       [ 6.48071143e-01, -8.76929259e-02,  9.94932440e-02,\n","        -6.81081575e-01],\n","       [-8.94982098e-01, -2.94699680e-01, -1.16692838e-01,\n","         6.21604652e-01]])"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["X_test"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPTMvEubU1lWjJmlpBlczXD","mount_file_id":"1KAUXeE3_3APEJqR2V9SL1ng3oue6nc8U","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
